                                  基于Transformer模型对以太坊价格预测
					
1. 背景:
   Transformer模型因其自注意力机制而在序列建模任务中表现出色，被用于语言处理等领域，此处尝试将其应用于以太坊价格预测任务。

2. 数据准备：
   https://api.coingecko.com/api/v3/coins/ethereum/history?date=

3. 模型构建：
   输入层的大小为3，对应着每个样本的特征数，这里是以太坊的交易量，市值和价格数据。
   使用了一个线性层将输入特征维度从3转换为256维度。
   采用了nn.TransformerEncoderLayer，包括隐藏层大小为256，注意力头数为8，dropout为0.1。
   使用了nn.TransformerEncoder将编码器层堆叠起来形成整个编码器。
   同样采用了nn.TransformerDecoderLayer，参数与编码器层类似。
   使用nn.TransformerDecoder将解码器层堆叠起来形成整个解码器。
   最后通过一个线性层将隐藏层的输出转换为3维度的输出，对应着模型预测的价格数据。

4. 训练模型：
   选择了损失函数（Smooth L1 Loss）和优化器（Adam）来训练模型，数据被划分成64批量并送入模型进行训练，
   并增加价格的权重使模型更专注于价格的准确性。

6. 预测输出：
   ![](https://github.com/blockchainx/tm23projects/blob/main/%E9%99%88%E5%B0%8F%E9%BE%99/src/predictedchart.png)

7. 总结：
   掌握了PyTorch构建深度学习模型的技能，从模型定义、损失函数、优化器到数据处理的各个环节。
   学会了处理数据，包括加载、标准化以及调整数据维度，使其适应模型输入的需求。
   调整Transformer模型的超参数（如隐藏层大小、层数、注意力头数）深入了解了这些参数对模型性能的影响，以优化模型以提升预测能力。
